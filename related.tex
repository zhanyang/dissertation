\chapter{Related Work}
\label{chap:related}

This chapter discusses related work to differentiate the contribution of
this dissertation
over previous research on write-optimization, full-path indexing in file
systems, tree surgery and clones in file systems.

\section{Write-optimization}

\paragraph{Write-optimized dictionaries (WODs).}
The most commonly used WOD is the
log-structured merge tree (LSM-tree)~\cite{lsm}.
The LSM-tree partitions key/value pairs into levels whose sizes
grow exponentially.
New key/value pairs are put into the first level and gradually merged
into higher levels.
The original paper implements each level as a tree.
When one level is full, all key/value pairs in the level are merged into the
next level.

Nowadays, most LSM-tree implementations use the design of
LevelDB~\cite{leveldb}.
Instead of using a tree to store key/value pairs in one level,
LevelDB divides a level into multiple 4MiB SSTables and
keeps the key range of each SSTable in the metadata file.
When a level is full, LevelDB picks an SSTable in the level and merges the
SSTable with SSTables in the next level.

There are also cache-oblivious WODs.
The cache-oblivious lookahead array (COLA)~\citep{cola} stores
each level in an array
and  uses fractional cascading~\citep{fcascading} to improve query performance.
The xDict~\citep{xdict} is a cache-oblivious write-optimized dictionary with
asymptotic behavior similar to a \bet.
However, only prototypes exist for cache-oblivious WODs.

We use write-optimized \bets~\cite{bet} in our implementation.
Compared to LSM-trees, \bets put all key/value pairs in one tree and have
better asymptotic query performance~\cite{betlogin}.

\paragraph{Key/value stores.}
Many key/value stores use WODs as the underlying data structures.
BigTable~\citep{bigtable}, Cassandra~\citep{cassandra}, HBase~\citep{hbase},
LevelDB~\citep{leveldb} and RocksDB~\citep{rocksdb} implement LSM-trees,
while \fti~\cite{fti} and Tucana~\cite{tucana} implement \bets.

Many papers focus on optimizing different aspects of the LSM-tree in LevelDB.
LOCS~\citep{locs} optimizes LSM-trees for multi-channel SSDs.
WiscKey~\citep{wisckey} reduces the size of the LSM-tree
so that the LSM-tree fits into the in-memory cache.
PebblesDB~\citep{pebble} optimizes the implementation of LSM-trees in LevelDB to
reduce write-amplification.

In this dissertation,
we provide the upper level file system with key/value store operations.
Specifically, for a namespace operation, the file system calls a key/value
store operation that finishes most the work efficiently.

\paragraph{File systems.}
The log-structured file system~\citep{lfs} treats the whole disk as a log,
thus it performs writes much faster than other file systems.
However, garbage collection can be expensive on the file system.

All previous write-optimized file systems are based on FUSE~\citep{fuse} so that
the authors can write the file system in userspace.
However, FUSE imposes expensive context-switch costs on the file system.

TokuFS~\citep{tokufs} is an in-application library file system, also built using
\bets.
TokuFS showed that a write-optimized file system can support efficient
write-intensive and scan-intensive workloads.
TokuFS had a FUSE-based version, but the authors explicitly omitted
disappointing measurements using FUSE.

KVFS~\citep{kvfs} is based on a transactional variation of an LSM-tree, called a
VT-tree.
Impressively, the performance of their transactional, FUSE-based file system was
comparable to the performance of the in-kernel ext4 file system,
which does not support transactions.
One performance highlight was on random-writes, where they outperformed ext4 by
a factor of 2.
They also used stitching to perform well on sequential I/O in the presence of
LSM-tree compaction.

TableFS~\citep{tablefs} uses LevelDB to store file-system metadata.
They showed substantial performance improvements on metadata-intensive
workloads, sometimes up to an order of magnitude.
They used ext4 as an object store for large files, so sequential IO performance
was com-parable to ext4.
They also analyzed the FUSE overhead relative to a library implementation of
their file system and found that FUSE could cause a 1000Ã— increase in disk-read
traffic.

\section{Full-path indexing in file systems}
A number of systems store metadata in a hash table, keyed by full-paths, to look
up metadata in one IO.
The Direct Lookup File System(DLFS) maps file metadata to on-disk buckets by
hashing full-paths~\citep{dlfs}.
Hashing full-paths creates two challenges, files in the same directory may be
scattered across disk, harming locality and DLFS directory renames require deep
recursive copies of both data and metadata.

A number of distributed file systems have stored file metadata in a hash table,
keyed by full-paths~\citep{gfs,nsidw,rdfs}.
In a distributed system, using a hash table for metadata has the advantage of
easy load balancing across nodes, as well as fast lookups
The concerns of indexing metadata in a distributed file system are quite
different from keeping logically contiguous data physically contiguous on disk.
Some systems, such as the Google File System, also do not support common POSIX
operations, such as listing a directory.

The dcache optimization proposed by Tsai et al. demonstrates that
indexing the in-memory kernel cache by full-path names can improve several
lookup operations, such as \texttt{open}~\citep{dcache}.

\section{Tree surgery}

Most tress used in storage systems only modify or rebalance node as the result
of insertions and deletions.
Operations, such as slicing out or relocating a subtree in tree surgery,
are uncommon.

Order Indexes~\citep{orderindex} introduces relocation updates which moves nodes
in the tree, to support dynamic indexing.

Ceph~\citep{ceph} performs dynamic subtree partitioning~\citep{cephtree} on the
directory tree to adaptively distribute metadata to different metadata servers.

\section{Clones in file systems}

\begin{table}[t]
    \centering
    \begin{tabular}{l | c c c }
        \hline
        File System & Whole File System Clone & File Clone & Directory Clone \\
        \hline
        \hline
        WAFL~\cite{wafl,wafl-flexvol} & Writable & No & No \\
        \hline
        FFS~\cite{ffs1,ffs2} & Read-only & No & No \\
        \hline
        NILFS2~\cite{nilfs2} & Read-only & No & No \\
        \hline
        ZFS~\cite{zfs} & Read-only & No & No \\
        \hline
        GCTree~\cite{gctree} & Read-only & No & No \\
        \hline
        NOVA-Fortis~\citep{nova} & Read-only & No & No \\
        \hline
        The Episode file system~\cite{episode} & No & No & Writable$^{*}$ \\
        \hline
        Ext3cow~\cite{ext3cow} & No & Writable & No \\
        \hline
        Btrfs~\citep{btrfs,cowbtree} & Writable & Writable & Writable$^{*}$ \\
        \hline
        EFS~\cite{efs} & No & Read-only & Read-only \\
        \hline
        CVFS~\cite{cvfs} & No & Read-only & Read-only \\
        \hline
        Versionfs~\citep{versionfs} & No & Read-only & Read-only \\
        \hline
        \betrfs & Writable & Writable & Writable \\
        \hline
    \end{tabular}
    \caption[Clones in file systems]{\label{tab:clonefs}
        Clones in file systems
        ($^{*}$ to enable directory clones, users need to mark the
        directory as a subvolume (Btrfs) or a fileset (the Episode file system)
        before putting anything to the directory).}
\end{table}

Table~\ref{tab:clonefs} summarizes the support of clones in file systems.

\paragraph{File systems with snapshots.}

Many modern file systems provide snapshot mechanism, making read-only copies of
the whole file system.

The WAFL file system~\citep{wafl} organizes all blocks in a tree structure.
By copying the vol\_info block, which is the root of the tree structure, WAFL
creates a snapshot.
Later, WAFL introduces a level of indirection between the file system and the
underlying disks~\citep{wafl-flexvol}.
Therefore, multiple active instances of the file system can exist at the same
time and WAFL can create writable snapshots of the whole file system by creating
a new instance and copying the vol\_info block.

FFS~\citep{ffs1,ffs2} creates snapshots by suspending all operations and creating a
snapshot file whose inode is stored in the superblock.
The size of the snapshot file is the same as the disk.
Upon creation, most block pointers in the snapshot inode are marked
``not copied'' or ``not used'' while some metadata blocks are copied to new
addresses.
Reading a ``not copied'' address in the snapshot file results in reading the
address on the disk.
When a ``not copied'' block is modified in the file system, FFS copies the block
to a new address and updates the block pointer in the snapshot inode.

NILFS~\citep{nilfs2} is a log-structured file system that organizes all blocks
in a B-tree.
In NILFS, each logical segment contains modified blocks and a checkpoint block
used as the root of the B-tree.
NILFS gets the current view of the file system from the checkpoint block
of the last logical segment.
NILFS can create a snapshot by making a checkpoint block permanent.

ZFS~\citep{zfs} also stores the file system in a tree structure and creates
snapshots by copying the root of the tree (uberblock).
To avoid maintaining one block bitmap for each snapshot, ZFS keeps birth time
in each pointer.
A block should not be freed if its birth time is earlier than any snapshot.
In that case, the block is added to the dead list of the most recent snapshot.
When a snapshot is deleted, all blocks in its dead list are checked again before
being freed.

GCTree~\citep{gctree} implements snapshots on top of ext4.
It chains different versions of a metadata block with GCTree metadata.
Also, each pointer in the metadata block has a ``borrowed bit'' indicating
whether the target block is inherited from the previous version.
Therefore, GCTree can check whether to free a block by inspecting GCTree
metadata and doesn't need to maintain reference counts.

NOVA-Fortis~\citep{nova} is designed for non-volatile main memory (NVMM).
In NOVA-Fortis, each inode has a private log with log entries pointing
to data blocks.
To enable snapshots, NOVA-Fortis keeps a global snapshot ID and adds the
creating snapshot ID to log entries in inodes.
NOVA-Fortis decides whether to free a block based on the snapshot ID in the log
entry and active snapshots.
It also deals with DAX-style \texttt{mmap} by stalling page faults when marking
all pages read-only.

\paragraph{File systems with file or directory clones.}

Some file systems go further to support cloning one single directory or file.

The Episode file system~\citep{episode} groups everything under a directory into
a fileset.
Episode can create an immutable fileset clone by copying all the anodes (inodes)
and marking all block pointers in the anodes copy-on-write.
When Episode changes a block with a copy-on-write pointer, it allocates a new
block and updates the block pointer in the active fileset.

Ext3cow~\citep{ext3cow} creates immutable file clones by maintaining a
system-wide epoch and inode epochs.
When an inode is modified, ext3cow allocates a new inode if the epoch in the
inode is older than the last snapshot epoch.
Also, each directory stores birth and death epochs for each entry.
Ext3cow can render the view of the file system in a certain epoch by fetching
entries alive at that epoch.

Btrfs~\citep{btrfs} supports creating writable snapshot of the whole file system
by copying the root of the sub-volume tree in its COW friendly
B-trees~\citep{cowbtree}.
Btrfs clones a file by sharing all extents of the file.
The extent allocation tree records extents with back pointers to the inodes.
Therefore, Btrfs is able to move an extent at a later time.

\paragraph{Versioning file systems}

There are also versioning file systems that versions files and directories.

EFS~\citep{efs} automatically versions files and directories.
By allocating a new inode, it creates and finalizes a new file version when the
file is opened and closed.
Each versioned file has an inode log that keeps all versions of the file.
All entries in directories have creation and deletion time.

CVFS~\citep{cvfs} tries to store metadata versions more compactly.
CVFS suggests two ways to save space in versioning file systems:
1. journal-based metadata that keeps a current version and an undo log to
recover previous versions;
2. multiversion B-trees that keep all versions of metadata in a B-tree.

Versionfs~\citep{versionfs} builds a stackable versioning file system.
Instead of building a new file system, Versionfs adds the functionality of
versioning on an existing file system by transferring certain operations to
other operations.
In this way, all versions of a file are maintained as different files in the
underlying file system.
