\chapter{Evaluation}
\label{chap:eval}

This chapter evaluates the performance of \betrfs with
range-rename (\betrfsFour) and \betrfs with range-clone (\betrfsFive).
The evaluation includes the following four aspects:
(1) performance of single file-system operations;
(2) performance of widely used applications;
(3) performance of renames;
(4) performance of clones.

\paragraph{Experimental Setup.}

All experimental results were collected on
a Dell Optiplex 790 with a 4-core 3.40 GHz Intel Core i7-2600 CPU,
4GB RAM,
and a 500 GB, 7200 SATA disk with a 4096-byte block size(Seagate Barracuda ST500DM002).
The system runs 64-bit Ubuntu server 14.04.05 on a USB stick to prevent
interference form the root file system.
\betrfsFour runs on a modified Linux-3.11.10 kernel that enlarges the size of the kernel stack,
while \betrfsFive and all other file systems run on Linux-4.9.142 kernel.
The evaluation uses ZFS 0.6.5.11 from \url{zfsonlinx.org} and
ext4, Btrfs, XFS and NILFS2 as parts of the Linux kernel.
Each experiment runs a minimum of 5 times and reports the median number.
Error bars indicate minimum and maximum numbers over all runs.
Similarly, error $\pm$ terms bound minimum and maximum numbers over all runs.
Unless noted, all benchmarks are cold-cache tests.

\section{Microbenchmarks}

\paragraph{Sequential writes and reads.}

We measure the throughput of sequentially writing and reading a file.
This benchmark first writes a 10-GiB file, 64 blocks at a time, with a
\texttt{fsync} to flush the file to disk.
Then, after cleaning the kernel page cache, the kernel reads the file from disk.

\newcommand{\addSeqPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=op,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/seq_io.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar,
                ymin=0,
                ylabel={Bandwidth (MiB/sec)},
                ymajorgrids=true,
                symbolic x coords={seq.write,seq.read},
                xtick={seq.write,seq.read},
                xticklabels={write,read},
                enlarge x limits=0.5,
                visualization depends on=y \as \rawy,
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                nodes near coords style={font=\small,anchor=east,rotate=90,xshift=-\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addSeqPlot{ext4};
            \addSeqPlot{btrfs};
            \addSeqPlot{xfs};
            \addSeqPlot{zfs};
            \addSeqPlot{nilfs2};
            \addSeqPlot{betrfs4};
            \addSeqPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Sequential-write and sequential-read benchmark]{\label{fig:seq_io}
        Bandwidth to sequentially read and write a 10 GiB file (higher is better).}
\end{figure}

Figure~\ref{fig:seq_io} shows the results.
Ext4, Btrfs, XFS performs sequential
writes close to disk bandwidth, while \betrfsFive, similar to NILFS2, is about
6.5\% slower than the fastest file system.
The performance increase of \betrfsFive from \betrfsFour is from preferential
splitting, which creates a pivot matching the maximum file data key the
beginning of the workload, avoiding further node relifting in subsequent node
splits.
For sequential reads, Ext4, Btrfs, XFS run at disk bandwidth, while \betrfsFive
is 19.1\% slower than the fastest file system, which is close to \betrfsFour
and NILFS2.
\betrfs reads a leaf, which is about 4 MiB in size, each time, while
extent-based file systems can have extents whose size is more than 100 MiB.
Thus, sequential reads results in more (and smaller) IOs on \betrfs.

\paragraph{Random writes.}

We then measure the performance of random writes on the file generated by the
sequential write benchmark.
The benchmark issues 256Ki 4-Byte overwrites (in total 1 MiB data) at random
offsets within the 10GiB file, followed by an \texttt{fsync}.

\newcolumntype{.}{D{.}{.}{-1}}

\begin{table}[t]
    \centering
    \begin{tabular}{l.@{${}\pm{}$}.}
        \hline
        File system & \multicolumn{2}{c}{random write (sec)} \\
        \hline
        \input{./data/rand_io.csv}
        \hline
    \end{tabular}
    \caption[Random-write benchmark]{\label{tab:rand_io}
        Time to perform 256Ki 4-Byte random writes one a 10GiB file (1 MiB total IO, lower is better).}
    \begin{tabular}{l.@{${}\pm{}$}..@{${}\pm{}$}..@{${}\pm{}$}.}
    \hline
    File system & \multicolumn{2}{c}{find (sec)} & \multicolumn{2}{c}{grep (sec)} & \multicolumn{2}{c}{delete (sec)} \\
    \hline
    \input{./data/dir_ops.csv}
    \hline
    \end{tabular}
    \caption[Directory operation benchmark]{\label{tab:dir_ops}
        Time to perform recursive grep, find and delete of the Linux 3.11.10 source directory (lower is better).}
\end{table}

Table~\ref{tab:rand_io} shows the results.
Because \betrfs performs upserts, which doesn't read the old data, for random
writes, performing the 1MiB random writes only takes about 5 seconds on
\betrfsFour and \betrfsFive.
However, it takes at least 2022 seconds on other file systems, which is
more than 400 times slower.

\newcommand{\addTokubenchPlot}[1]
{
    \addplot[
        color=\pgfkeysvalueof{/fs-colors/#1},
        line width=0.75pt,
        mark=\pgfkeysvalueof{/fs-marks/#1},
    ]
    plot[
    ]
    table[
    ]
    {./data/toku/#1.csv};
    \addlegendentry{\pgfkeysvalueof{/fs-names/#1}}
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                xlabel={Files created},
                ylabel={Throughput (files/sec)},
                xmin=0,
                xmax=3000000,
                ymin=10,
                ymax=50000,
                mark repeat=10,
                ytick={10000,20000,30000,40000,50000},
                yticklabels={10k,20k,30k,40k,50k},
                xtick={0,1000000,2000000,3000000},
                xticklabels={0,1M,2M,3M},
                grid=major,
                scaled x ticks=false,
                scaled y ticks=false,
                legend columns=4,
                legend cell align=left,
                transpose legend,
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addTokubenchPlot{ext4};
            \addTokubenchPlot{btrfs};
            \addTokubenchPlot{xfs};
            \addTokubenchPlot{zfs};
            \addTokubenchPlot{nilfs2};
            \addTokubenchPlot{betrfs4};
            \addTokubenchPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[TokuBench benchmark]{\label{fig:toku}
        Cumulative file creation throughput during the Tokubench benchmark (higher is better).}
\end{figure}

\paragraph{Directory operations.}
Next, we measure three common directory operations,
\texttt{grep}, \texttt{find}, and ``rm -rf''.
The benchmark measures the time to \texttt{grep} keyword cpu\_to\_be64 and
\texttt{find} file wait.c on a copy of the Linux 3.11.10 source directory.
Also, it measure the time to recursively delete the directory with ``rm -rf''.

Table~\ref{tab:dir_ops} shows the results.
Because full-path indexing ensures locality in \betrfs, \texttt{find} and
\texttt{grep} on \betrfsFour and \betrfsFive are more than two times faster than
other file systems.
Recursive delete is implemented by range-delete messages in \betrfsFour and
\goto messages in \betrfsFive, both shows comparable performance against other
file systems.

\paragraph{TokuBench.}

TokuBench is a small-write-intensive benchmark that creates 3 million
200-Byte files in a balanced tree directory.
The benchmark first creates the balanced tree directory with a fanout of 128,
i.e., each directory has at most 128 directories or 128 files.
Then, it creates files with 4 threads.
And each thread iterates over the leaf directories, creating one file at a time.
The benchmark reports the cumulative throughput of the file creation each time
10000 files are created.

Figure~\ref{fig:toku} shows the results.
Because Tokubench focus on small, random writes, \betrfsFour and \betrfsFive are
significantly faster than ext4, Btrfs and XFS.
\betrfsFive performance better than \betrfsFour because preferential splitting
also avoids further relifting in the benchmark.
We expect NILFS to performance better than \betrfsFive in TokuBench that creates
more files,
because as a log-structured file system, NILFS2 would have stable performance
until the log (disk) is full.

\section{Macrobenchmarks}

We then measure the performance of file systems on four canonical applications.

\paragraph{Git.}

\paragraph{Tar.}

\paragraph{Rsync.}

\paragraph{Mailserver.}

\section{Rename benchmarks}

\textbf{TODO}

\section{Clone benchmarks}

\textbf{TODO}

\section{Summary}
