\chapter{Related Work}
\label{chap:related}

This chapter discusses related work to differentiate the contribution of
this dissertation
over previous research on write-optimization, full-path indexing in file
systems, tree surgery and clones in file systems.

\section{Write-optimization}

\paragraph{Write-optimized dictionaries (WODs).}
The most commonly used WOD is the
log-structured merge tree (LSM-tree)~\cite{lsm}.
The LSM-tree partitioned key/value pairs into levels
whose sizes grew exponentially.
Each level in the LSM-tree was an \btree.
The LSM-tree put new key/value pairs into the first level.
When a level was full, the LSM-tree merged all key/value pairs in the level
into the next level.

Nowadays, most LSM-tree implementations use the design of
LevelDB~\cite{leveldb}.
Instead of using a tree to store key/value pairs in one level,
LevelDB divides a level into multiple 4MiB SSTables and
keeps the key range of each SSTable in the metadata file.
When a level is full, LevelDB picks an SSTable in the level and merges the
SSTable with SSTables in the next level.

There were also cache-oblivious WODs.
The cache-oblivious lookahead array (COLA)~\citep{cola} stored
each level in an array
and used fractional cascading~\citep{fcascading} to improve query performance.
The xDict~\citep{xdict} was a cache-oblivious write-optimized dictionary with
asymptotic behavior similar to a \bet.
However, only prototypes exist for cache-oblivious WODs.

We use write-optimized \bets~\cite{bet} in our implementation.
Compared to LSM-trees, \bets put all key/value pairs in one tree and have
better asymptotic query performance~\cite{betlogin}.

\paragraph{Key/value stores.}
Many key/value stores use WODs as the underlying data structures.
BigTable~\citep{bigtable}, Cassandra~\citep{cassandra}, HBase~\citep{hbase},
LevelDB~\citep{leveldb} and RocksDB~\citep{rocksdb} implement LSM-trees,
while \fti~\cite{fti} and Tucana~\cite{tucana} implement \bets.

Many papers focused on optimizing different aspects of the LSM-tree in LevelDB.
LOCS~\citep{locs} optimized LSM-trees for multi-channel SSDs.
WiscKey~\citep{wisckey} reduced the size of the LSM-tree
so that the LSM-tree fitted into the in-memory cache.
PebblesDB~\citep{pebble} optimized the implementation of LSM-trees in LevelDB to
reduce write-amplification.

In this dissertation,
we provide the upper level file system with key/value store operations.
Specifically, for a namespace operation, the file system calls a key/value
store operation that finishes most of the work efficiently.

\paragraph{File systems.}
The log-structured file system~\citep{lfs} treated the whole disk as a log.
Thus, it performed writes much faster than other file systems.
However, garbage collection could be expensive in the file system~\cite{lfsbsd}.

TokuFS~\citep{tokufs} was an in-application library file system,
built on top of \fti.
TokuFS showed that a write-optimized file system could support efficient
write-intensive and scan-intensive workloads.

KVFS~\citep{kvfs} was based on a transactional variation of the LSM-tree,
called the VT-tree.
Impressively, the performance of their transactional file system was
comparable to the performance of the ext4 file system,
which does not support transactions.
One performance highlight was on random-writes, where they outperformed ext4 by
a factor of 2.
They also used stitching to perform well on sequential I/O in the presence of
LSM-tree compaction.

TableFS~\citep{tablefs} used LevelDB to store file-system metadata.
They showed substantial performance improvements on metadata-intensive
workloads, sometimes up to an order of magnitude.
They used ext4 as an object store for large files, so sequential IO performance
was com-parable to ext4.

All the WOD-based file systems above were built on FUSE~\citep{fuse},
in which the authors can write file systems in userspace.
However, FUSE imposes expensive context-switch costs on the file system.
The TableFS paper analyzed the FUSE overhead relative to a library
implementation of their file system and
found that FUSE could cause a 1000Ã— increase in disk-read traffic.

\betrfs~\cite{betrfs1,betrfs1tos,betrfs2,betrfs2tos,betrfs3} was the first
WOD-based file system in kernel.
Because of the underlying WOD, \betrfs performed well on benchmarks
with small, random writes.

The implementation in this dissertation is a newer version of \betrfs,
which inherits the good random write performance from older versions and
has a new design for namespace operations.

\section{Full-path indexing in file systems}
A number of systems store metadata in a hash table,
keyed by full-paths, to look up metadata in one IO.
The Direct Lookup File System(DLFS) mapped file metadata to on-disk buckets by
hashing full-paths~\citep{dlfs}.
Hashing full-paths created two challenges, files in the same directory might be
scattered across disk, harming locality and DLFS directory renames required deep
recursive copies of both data and metadata.

A number of distributed file systems have stored file metadata in a hash table,
keyed by full-paths~\citep{gfs,nsidw,rdfs}.
In a distributed system, using a hash table for metadata has the advantage of
easy load balancing across nodes, as well as fast lookups
The concerns of indexing metadata in a distributed file system are quite
different from keeping logically contiguous data physically contiguous on disk.
Some systems, such as the Google File System, also do not support common POSIX
operations, such as listing a directory.

The dcache optimization proposed by Tsai et al. demonstrated that
indexing the in-memory kernel cache by full-path names can improve several
lookup operations, such as \texttt{open}~\citep{dcache}.
In their system, a rename needed to invalidate all related caches.

The first version of \betrfs~\cite{betrfs1,betrfs1tos} used full-path indexing
and renames in the first version of \betrfs could be very slow.
Therefore, later versions of \betrfs~\cite{betrfs2,betrfs2tos,betrfs3} used
relative-path indexing.
Relative-path indexing divided the directory tree into zones and used
full-path indexing within a zone.
Relative-path indexing showed good rename performance but penalized other
operations.

This dissertation brings full-path indexing back to \betrfs
and implements efficient file system renames by doing tree surgery on \bets.

\section{Tree surgery}

Most trees used in storage systems only modify or rebalance node as the result
of insertions and deletions.
Operations, such as slicing out or relocating a subtree in tree surgery,
are uncommon.

Finis et al.~\citep{orderindex} introduced ordered index for handling
hierarchical data, such as XML, in relational database systems.
Similar to a file system rename that moves a subtree in the directory hierarchy,
one of their task was moving a subtree in the hierarchical data.
However, in their use cases, hierarchy indexes were generally secondary
indexes,
while in \betrfs, full-path keys encode the directory hierarchy.

Ceph~\citep{ceph} used dynamic subtree partitioning~\citep{cephtree}
to load balancing metadata servers.
In Ceph, a busy metadata server would delegate subtrees of its workload to other
metadata servers.
Compared to namespace operations of this dissertation,
the dynamic subtree partitioning in Ceph didn't change the overall directory
hierarchy of the file system.

\section{Clones in file systems}

\begin{table}[t]
    \centering
    \begin{tabular}{l | c c c }
        \hline
        File System & Whole File System Clone & File Clone & Directory Clone \\
        \hline
        \hline
        WAFL~\cite{wafl,wafl-flexvol} & Writable & No & No \\
        \hline
        FFS~\cite{ffs1,ffs2} & Read-only & No & No \\
        \hline
        NILFS2~\cite{nilfs2} & Read-only & No & No \\
        \hline
        ZFS~\cite{zfs} & Read-only & No & No \\
        \hline
        GCTree~\cite{gctree} & Read-only & No & No \\
        \hline
        NOVA-Fortis~\citep{nova} & Read-only & No & No \\
        \hline
        The Episode file system~\cite{episode} & No & No & Writable$^{*}$ \\
        \hline
        Ext3cow~\cite{ext3cow} & No & Writable & No \\
        \hline
        Btrfs~\citep{btrfs,cowbtree} & Writable & Writable & Writable$^{*}$ \\
        \hline
        EFS~\cite{efs} & No & Read-only & Read-only \\
        \hline
        CVFS~\cite{cvfs} & No & Read-only & Read-only \\
        \hline
        Versionfs~\citep{versionfs} & No & Read-only & Read-only \\
        \hline
        \betrfs & Writable & Writable & Writable \\
        \hline
    \end{tabular}
    \caption[Clones in file systems]{\label{tab:clonefs}
        Clones in file systems
        ($^{*}$ to enable directory clones, users need to mark the
        directory as a subvolume (Btrfs) or a fileset (the Episode file system)
        before putting anything to the directory).}
\end{table}

We also investigate file or directory clones in this dissertation.
Table~\ref{tab:clonefs} summarizes the support of clones in file systems.

\paragraph{File systems with snapshots.}

Many modern file systems provide snapshot mechanism, making read-only copies of
the whole file system.

The WAFL file system~\citep{wafl} organized all blocks in a tree structure.
By copying the ``vol\_info'' block, which was the root of the tree structure,
WAFL created a snapshot.
Later, WAFL introduced a level of indirection between the file system and the
underlying disks~\citep{wafl-flexvol}.
Therefore, multiple active instances of the file system could exist at the same
time and WAFL could create writable snapshots of the whole file system by creating
a new instance and copying the vol\_info block.

FFS~\citep{ffs1,ffs2} created snapshots by suspending all operations and creating a
snapshot file whose inode was stored in the superblock.
The size of the snapshot file was the same as the disk.
Upon creation, most block pointers in the snapshot inode were marked
``not copied'' or ``not used'' while some metadata blocks were copied to new
addresses.
Reading a ``not copied'' address in the snapshot file resulted in reading the
address on the disk.
When a ``not copied'' block was modified in the file system, FFS copied the block
to a new address and updated the block pointer in the snapshot inode.

NILFS~\citep{nilfs2} was a log-structured file system that organizes all blocks
in a B-tree.
In NILFS, each logical segment contained modified blocks and a checkpoint block,
which was used as the root of the B-tree.
NILFS got the current view of the file system from the checkpoint block
of the last logical segment.
NILFS could create a snapshot by making a checkpoint block permanent.

ZFS~\citep{zfs} also stored the file system in a tree structure and created
snapshots by copying the root of the tree (uberblock).
To avoid maintaining one block bitmap for each snapshot, ZFS kept birth time
in each pointer.
A block should not be freed if its birth time was earlier than any snapshot.
In that case, the block was added to the dead list of the most recent snapshot.
When a snapshot was deleted,
all blocks in its dead list were checked again before being freed.

GCTree~\citep{gctree} implemented snapshots on top of ext4
by chaining different versions of a metadata block with GCTree metadata.
Also, each pointer in the metadata block had a ``borrowed bit'' indicating
whether the target block was inherited from the previous version.
Therefore, GCTree could check whether to free a block by inspecting GCTree
metadata and didn't need to maintain reference counts.

NOVA-Fortis~\citep{nova} was designed for non-volatile main memory (NVMM).
In NOVA-Fortis, each inode had a private log with log entries pointing
to data blocks.
To enable snapshots, NOVA-Fortis kept a global snapshot ID and added the
creating snapshot ID to log entries in inodes.
NOVA-Fortis decided whether to free a block based on the snapshot ID in the log
entry and active snapshots.
NOVA-Fortis also dealt with DAX-style \texttt{mmap}
by stalling page faults when marking all pages read-only.

Snapshots in file systems provide the basic functionality of cloning files
and directories in a coarse granularity.
However, users may want to clone only a certain file or directory,
in which cloning the whole file system can waste time and space.

\paragraph{File systems with file or directory clones.}

Some file systems go further to support cloning one single file or directory.

The Episode file system~\citep{episode} grouped everything under a directory
into a fileset.
Episode could create an immutable fileset clone by copying all the anodes
(inodes) and marking all block pointers in the anodes copy-on-write.
When modifying a block with a copy-on-write pointer, Episode allocated a new
block and updated the block pointer in the active fileset.

Ext3cow~\citep{ext3cow} created immutable file clones by maintaining a
system-wide epoch and inode epochs.
When an inode was modified, ext3cow allocated a new inode if the epoch in the
inode was older than the last snapshot epoch.
Also, each directory stored birth and death epochs for each entry.
Ext3cow could render the view of the file system in a certain epoch by fetching
entries alive at that epoch.

Btrfs~\citep{btrfs} supported creating writable snapshot of
the whole file system
by copying the root of the sub-volume tree in its COW friendly
B-trees~\citep{cowbtree}.
Btrfs cloned a file by sharing all extents of the file.
The extent allocation tree recorded extents with back pointers to the inodes.
Therefore, Btrfs was able to move an extent at a later time.

All existing file systems either don't support directory clones
or require pre-configuration for directory clones.
Or, a file system with file clones can clone a directory by traversing the
directory and cloning each file,
which can be costly if the directory is huge.

\paragraph{Versioning file systems}

There are also versioning file systems that versions files and directories.

EFS~\citep{efs} automatically versioned files and directories.
By allocating a new inode, EFS created and finalized a new file version when the
file was opened and closed.
Each versioned file had an inode log that keeps all versions of the file.
All entries in directories had creation and deletion time.

CVFS~\citep{cvfs} tried to store metadata versions more compactly.
CVFS suggested two ways to save space in versioning file systems:
1. journal-based metadata that kept a current version and an undo log to
recover previous versions;
2. multiversion B-trees that kept all versions of metadata in a B-tree.

Versionfs~\citep{versionfs} built a stackable versioning file system.
Instead of building a new file system, Versionfs added the functionality of
versioning on an existing file system by transferring certain operations to
other operations.
In this way, all versions of a file were maintained as different files in the
underlying file system.

Versioning file systems study how to automatically provide file or directory
clones for users,
while this dissertation focuses on how to implement file or directory clones
in a full-path-indexed file system.

