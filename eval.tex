\chapter{Evaluation}
\label{chap:eval}

This chapter evaluates the performance of \betrfs with
range-rename (\betrfsFour) and \betrfs with range-clone (\betrfsFive).
The evaluation includes the following four aspects:
(1) performance of single file-system operations;
(2) performance of widely used applications;
(3) performance of renames;
(4) performance of clones.

\paragraph{Experimental Setup.}

All experimental results were collected on
a Dell Optiplex 790 with a 4-core 3.40 GHz Intel Core i7-2600 CPU,
4GB RAM,
and a 500 GB, 7200 SATA disk with a 4096-byte block size(Seagate Barracuda ST500DM002).
The system runs 64-bit Ubuntu server 14.04.05 on a USB stick to prevent
interference form the root file system.
\betrfsFour runs on a modified Linux-3.11.10 kernel that enlarges the size of the kernel stack,
while \betrfsFive and all other file systems run on Linux-4.9.142 kernel.
The evaluation uses ZFS 0.6.5.11 from \url{zfsonlinx.org} and
ext4, Btrfs, XFS and NILFS2 as parts of the Linux kernel.
Each experiment runs a minimum of 5 times and reports the median number.
Error bars indicate minimum and maximum numbers over all runs.
Similarly, error $\pm$ terms bound minimum and maximum numbers over all runs.
Unless noted, all benchmarks are cold-cache tests.

\section{Microbenchmarks}

\textbf{TODO: on aged file systems.}

\newcommand{\addSeqPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=op,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/seq_io.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar,
                ymin=0,
                ylabel={Bandwidth (MiB/sec)},
                ymajorgrids=true,
                symbolic x coords={seq.write,seq.read},
                xtick={seq.write,seq.read},
                xticklabels={write,read},
                enlarge x limits=0.5,
                visualization depends on=y \as \rawy,
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                nodes near coords style={font=\small,anchor=east,rotate=90,xshift=-\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addSeqPlot{ext4};
            \addSeqPlot{btrfs};
            \addSeqPlot{xfs};
            \addSeqPlot{zfs};
            \addSeqPlot{nilfs2};
            \addSeqPlot{betrfs4};
            \addSeqPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Sequential-write and sequential-read benchmark]{\label{fig:seq_io}
        Bandwidth to sequentially read and write a 10 GiB file (higher is better).}
\end{figure}

\newcolumntype{.}{D{.}{.}{-1}}

\begin{table}[t]
    \centering
    \begin{tabular}{l|.@{${}\pm{}$}.}
        \hline
        File system & \multicolumn{2}{c}{random write (sec)} \\
        \hline
        \hline
        \input{./data/rand_io.csv}
        \hline
    \end{tabular}
    \caption[Random-write benchmark]{\label{tab:rand_io}
        Time to perform 256Ki 4-Byte random writes one a 10GiB file (1 MiB total IO, lower is better).}
    \begin{tabular}{l|.@{${}\pm{}$}..@{${}\pm{}$}..@{${}\pm{}$}.}
    \hline
    File system & \multicolumn{2}{c}{find (sec)} & \multicolumn{2}{c}{grep (sec)} & \multicolumn{2}{c}{delete (sec)} \\
    \hline
    \hline
    \input{./data/dir_ops.csv}
    \hline
    \end{tabular}
    \caption[Directory operation benchmark]{\label{tab:dir_ops}
        Time to perform recursive grep, find and delete of the Linux 3.11.10 source directory (lower is better).}
\end{table}

\newcommand{\addTokubenchPlot}[1]
{
    \addplot[
        color=\pgfkeysvalueof{/fs-colors/#1},
        line width=0.75pt,
        mark=\pgfkeysvalueof{/fs-marks/#1},
    ]
    plot[
    ]
    table[
    ]
    {./data/toku/#1.csv};
    \addlegendentry{\pgfkeysvalueof{/fs-names/#1}}
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                xlabel={Files created},
                ylabel={Throughput (files/sec)},
                xmin=0,
                xmax=3000000,
                ymin=10,
                ymax=50000,
                mark repeat=10,
                ytick={10000,20000,30000,40000,50000},
                yticklabels={10k,20k,30k,40k,50k},
                xtick={0,1000000,2000000,3000000},
                xticklabels={0,1M,2M,3M},
                grid=major,
                scaled x ticks=false,
                scaled y ticks=false,
                legend columns=4,
                legend cell align=left,
                transpose legend,
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addTokubenchPlot{ext4};
            \addTokubenchPlot{btrfs};
            \addTokubenchPlot{xfs};
            \addTokubenchPlot{zfs};
            \addTokubenchPlot{nilfs2};
            \addTokubenchPlot{betrfs4};
            \addTokubenchPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[TokuBench benchmark]{\label{fig:toku}
        Cumulative file creation throughput during the Tokubench benchmark (higher is better).}
\end{figure}

\newcommand{\addGitPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=op,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/git.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar,
                ymin=0,
                ylabel={Time (sec)},
                ymajorgrids=true,
                symbolic x coords={clone, diff},
                xticklabels={git clone,git diff},
                xtick={clone,diff},
                enlarge x limits=0.5,
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                visualization depends on=y \as \rawy,
                nodes near coords style={font=\large,anchor=east,rotate=90,xshift=-\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addGitPlot{ext4};
            \addGitPlot{btrfs};
            \addGitPlot{xfs};
            \addGitPlot{zfs};
            \addGitPlot{nilfs2};
            \addGitPlot{betrfs4};
            \addGitPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Git benchmark]{\label{fig:git}
        Latency of ``git clone'' and ``git diff'' (lower is better).}
\end{figure}

\newcommand{\addTarPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=op,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/tar.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar,
                ymin=0,
                ylabel={Time (sec)},
                ymajorgrids=true,
                symbolic x coords={untar,tar},
                xtick={untar,tar},
                enlarge x limits=0.5,
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                visualization depends on=y \as \rawy,
                nodes near coords style={font=\large,anchor=east,rotate=90,xshift=-10*\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addTarPlot{ext4};
            \addTarPlot{btrfs};
            \addTarPlot{xfs};
            \addTarPlot{zfs};
            \addTarPlot{nilfs2};
            \addTarPlot{betrfs4};
            \addTarPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Tar benchmark]{\label{fig:tar}
        Latency to untar and tar the Liinux-3.11.10 source directory (lower is better).}
\end{figure}

\newcommand{\addRsyncPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=inplace,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/rsync.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar,
                ymin=0,
                ylabel={Bandwidth (MB/sec)},
                ymajorgrids=true,
                symbolic x coords={yes,no},
                xtick={yes,no},
                xticklabels={\large{\texttt{--in-place}},\large{rename}},
                enlarge x limits=0.5,
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                visualization depends on=y \as \rawy,
                nodes near coords style={font=\large,anchor=east,rotate=90,xshift=-10*\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
        \addRsyncPlot{ext4};
        \addRsyncPlot{btrfs};
        \addRsyncPlot{xfs};
        \addRsyncPlot{zfs};
        \addRsyncPlot{nilfs2};
        \addRsyncPlot{betrfs4};
        \addRsyncPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Rsync benchmark]{\label{fig:rsync}
        Throughput of using rsync to clone the Linux-3.11.10 source directory (higher is better).}
\end{figure}

\newcommand{\addIMAPPlot}[1]{
    \addplot[
        discard if not={fs}{#1},
        fill=\pgfkeysvalueof{/fs-colors/#1},
        nodes near coords=\pgfkeysvalueof{/fs-names/#1},
    ]
    plot[
        error bars/.cd,
        y dir=both, y explicit,
    ]
    table[
        x=fs,
        y=median,
        y error plus expr=\thisrow{max}-\thisrow{median},
        y error minus expr=\thisrow{median}-\thisrow{min},
    ]
    {./data/imap.csv};
}

\begin{figure}[t]
    \begin{tikzpicture}[yscale=0.95, xscale=0.95]
        \begin{axis}[
                ybar=0pt,
                /pgf/bar shift=0pt,
                ymin=0,
                ylabel={Throughput (op/s)},
                ymajorgrids=true,
                symbolic x coords={ext4,btrfs,xfs,zfs,nilfs2,betrfs4,betrfs5,betrfs5-clone},
                xticklabels={},
                xtick pos=right,
                major tick length=0in,
                xticklabel pos=right,
                visualization depends on=y \as \rawy,
                nodes near coords style={font=\large,anchor=east,rotate=90,xshift=-\pgfplotsunitylength*\rawy,},
                height=.6\linewidth,
                width=\linewidth,
            ]
            \addIMAPPlot{ext4};
            \addIMAPPlot{btrfs};
            \addIMAPPlot{xfs};
            \addIMAPPlot{zfs};
            \addIMAPPlot{nilfs2};
            \addIMAPPlot{betrfs4};
            \addIMAPPlot{betrfs5};
        \end{axis}
    \end{tikzpicture}
    \caption[Mailserver benchmark]{\label{fig:imap}
        Throughput of the dovecot mailserver (higher is better).}
\end{figure}

\paragraph{Sequential writes and reads.}

We measure the throughput of sequentially writing and reading a file.
This benchmark first writes a 10-GiB file, 64 blocks at a time, with a
\texttt{fsync} to flush the file to disk.
Then, after cleaning the kernel page cache, the kernel reads the file from disk.

Figure~\ref{fig:seq_io} shows the results.
Ext4, Btrfs, XFS performs sequential
writes close to disk bandwidth, while \betrfsFive, similar to NILFS2, is about
6.5\% slower than the fastest file system.
The performance increase of \betrfsFive from \betrfsFour is from preferential
splitting, which creates a pivot matching the maximum file data key the
beginning of the workload, avoiding further node relifting in subsequent node
splits.
For sequential reads, Ext4, Btrfs, XFS run at disk bandwidth, while \betrfsFive
is 19.1\% slower than the fastest file system, which is close to \betrfsFour
and NILFS2.
\betrfs reads a leaf, which is about 4 MiB in size, each time, while
extent-based file systems can have extents whose size is more than 100 MiB.
Thus, sequential reads results in more (and smaller) IOs on \betrfs.

\paragraph{Random writes.}

We then measure the performance of random writes on the file generated by the
sequential write benchmark.
The benchmark issues 256Ki 4-Byte overwrites (in total 1 MiB data) at random
offsets within the 10GiB file, followed by an \texttt{fsync}.

Table~\ref{tab:rand_io} shows the results.
Because \betrfs performs upserts, which doesn't read the old data, for random
writes, performing the 1MiB random writes only takes about 5 seconds on
\betrfsFour and \betrfsFive.
However, it takes at least 2022 seconds on other file systems, which is
more than 400 times slower.

\paragraph{Directory operations.}
Next, we measure three common directory operations,
\texttt{grep}, \texttt{find}, and ``rm -rf''.
The benchmark measures the time to \texttt{grep} keyword cpu\_to\_be64 and
\texttt{find} file wait.c on a copy of the Linux 3.11.10 source directory.
Also, it measure the time to recursively delete the directory with ``rm -rf''.

Table~\ref{tab:dir_ops} shows the results.
Because full-path indexing ensures locality in \betrfs, \texttt{find} and
\texttt{grep} on \betrfsFour and \betrfsFive are more than two times faster than
other file systems.
Recursive delete is implemented by range-delete messages in \betrfsFour and
\goto messages in \betrfsFive, both shows comparable performance against other
file systems.

\paragraph{TokuBench.}

TokuBench is a small-write-intensive benchmark that creates 3 million
200-Byte files in a balanced tree directory.
The benchmark first creates the balanced tree directory with a fanout of 128,
i.e., each directory has at most 128 directories or 128 files.
Then, it creates files with 4 threads.
And each thread iterates over the leaf directories, creating one file at a time.
The benchmark reports the cumulative throughput of the file creation each time
10000 files are created.

Figure~\ref{fig:toku} shows the results.
Because Tokubench focus on small, random writes, \betrfsFour and \betrfsFive are
significantly faster than ext4, Btrfs and XFS.
\betrfsFive performance better than \betrfsFour because preferential splitting
also avoids further relifting in the benchmark.
We expect NILFS to performance better than \betrfsFive in TokuBench that creates
more files,
because as a log-structured file system, NILFS2 would have stable performance
until the log (disk) is full.

\section{Macrobenchmarks}

\textbf{TODO: on aged file systems.}

We then measure the performance of file systems on four canonical applications.

\paragraph{Git.}

The git benchmark first measures the latency of finishing ``git clone'' that
clones a local copy of the linux source repository,
then measures the latency of finishing ``git diff'' that generates a patch
between two tags in the repository, the ``v4.7'' and ``v4.14'' tags.

Figure~\ref{fig:git} shows the results.
The ``git clone'' benchmark performs a mixture sequential writes and file
creations.
\betrfsFour and \betrfsFive are slightly slower than ext4 and Btrfs mainly
because they have slower sequential writes.
The ``git diff'' benchmark traverses the directory and reads the deltas from git
objects.
\betrfsFour and \betrfsFive are among the fastest file systems because of good
locality in directory traversals.

\paragraph{Tar.}

The tar benchmark measures the latency of untar and tar.
The benchmark first copies a Linux-3.11.10 source tar ball to the file systems.
Then, it measures the time to untar the tar ball, which writes a Linux-3.11.10
source directory.
At last, it measures the time to generate a tar ball from the newly created
Linux-3.11.10 source directory.

Figure~\ref{fig:tar} shows the result.
Untaring consists sequentially reading the tar ball and file creations.
\betrfsFour and \betrfsFive are only slightly slower than Btrfs.
Taring traverse the directory and sequentially writes the tar ball.
\betrfsFour and \betrfsFive are the fastest file system in the tar benchmark.

\paragraph{Rsync.}

The rsync benchmark first creates a Linux-3.11.10 source directory in the file
system, then uses rsync to create a copy of the directory.
This benchmark runs twice on each file system, one with the ``in-place'' flags
and the other without the flag.
Without the flag, rsync would first creates temporary files and rename them to
be final files, while with the flag, rsync would creates files in place.

Figure~\ref{fig:rsync} shows the results.
\betrfsFour and \betrfsFive have much higher throughput than other file system
because part of the job is to traverse the old directory.

\paragraph{Mailserver.}

The mailserver benchmarks measures the througput of the dovecot mailserver on
different file systems.
The mailserver is configured with Maildir option, therefore, each mail is a
file.
Initially, it has 10 boxes, each with 1000 mails.
Then, the benchmark creates four threads that interact with the mailserver and
measure the throughput.
Each threads performs 50\% reads and 50\%write, writes are randomly chosen from
creating a new mail, deleting an existing mail and changing the flag of an
existing mail with equal probabilities.

Figure~\ref{fig:imap} shows the results.
Because the benchmark performs random small writes, \betrfsFour and \betrfsFive
are only slightly slower than NILFS2.

\section{Rename benchmarks}

\textbf{TODO}

\section{Clone benchmarks}

\textbf{TODO}

\section{Summary}
